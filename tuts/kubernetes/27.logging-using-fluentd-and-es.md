Kubernetes logging can be managed using Fluentd and Elasticsearch as a logging stack. Fluentd is an open-source data collector that can collect, process, and forward log data from different sources to various destinations, including Elasticsearch. Elasticsearch is an open-source search and analytics engine that can store and search large volumes of log data.

Here are the steps to set up Kubernetes logging with Fluentd and Elasticsearch:

1. Install and configure Fluentd DaemonSet: A DaemonSet is a Kubernetes object that ensures that a copy of a pod is running on each node in the cluster. Install Fluentd as a DaemonSet in the Kubernetes cluster to collect logs from the containers running on each node.

2. Configure Fluentd to forward logs to Elasticsearch: In the Fluentd configuration file, configure the Elasticsearch plugin to forward the collected logs to Elasticsearch. Provide the Elasticsearch host and port information and set up the index and document type for the logs.

3. Install Elasticsearch in the Kubernetes cluster: Install Elasticsearch as a stateful set in the Kubernetes cluster. Configure the Elasticsearch nodes to store the logs in the Elasticsearch index.

4. Configure Kibana for visualization: Kibana is an open-source visualization tool that can be used to visualize and analyze the logs stored in Elasticsearch. Install Kibana as a deployment in the Kubernetes cluster and configure it to access the Elasticsearch index.

5. Verify the logging stack: Verify that the logging stack is working correctly by accessing the Kibana UI and checking for log data. Search for log data using the Kibana search and filter options.

This logging stack provides a scalable and flexible way to manage Kubernetes logs. The Fluentd configuration can be customized to collect logs from different sources and to filter and transform the log data before forwarding it to Elasticsearch. The Elasticsearch index can be managed to ensure that log data is stored and managed efficiently. Kibana provides a powerful visualization and analysis tool to search and visualize the log data in various formats.

Here is an example of a Fluentd configuration file for collecting logs from Kubernetes pods and forwarding them to Elasticsearch:

```config
<source>
  @type tail
  path /var/log/containers/*.log
  pos_file /var/log/fluentd-containers.log.pos
  tag kubernetes.*
  read_from_head true
  <parse>
    @type json
    time_key time
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

<match kubernetes.**>
  @type elasticsearch
  hosts elasticsearch-cluster.default.svc.cluster.local:9200
  logstash_format true
  logstash_prefix kubernetes.logs
  flush_interval 5s
</match>
```

This configuration file sets up Fluentd to collect logs from the Kubernetes pod log files located at `/var/log/containers/*.log`. The collected logs are tagged with `kubernetes.*`. The `parse` section defines that the log data is in JSON format with a timestamp format of `%Y-%m-%dT%H:%M:%S.%NZ`.

The `match` section configures Fluentd to forward the logs to Elasticsearch using the Elasticsearch plugin. The Elasticsearch hosts are specified as `elasticsearch-cluster.default.svc.cluster.local:9200`. The `logstash_format` parameter ensures that the logs are formatted in the Logstash format, which Elasticsearch can parse easily. The `logstash_prefix` parameter defines the prefix for the Elasticsearch index that will store the logs.

With this configuration, Fluentd will collect logs from the Kubernetes pod log files and forward them to Elasticsearch, where they will be indexed and stored. The logs can then be visualized and analyzed using Kibana.